# This Python 3 environment comes with many helpful analytics libraries installed
# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python
# For example, here's several helpful packages to load in 

import numpy as np # linear algebra
import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)
import tensorflow as tf
import matplotlib.pyplot as plt
from sklearn.preprocessing import Imputer, LabelEncoder, MinMaxScaler, LabelBinarizer, Binarizer
from sklearn.model_selection import train_test_split
from collections import namedtuple

# Input data files are available in the "../input/" directory.
# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory

import os
os.listdir("../input")

# Any results you write to the current directory are saved as output.

# read the test, train csv
train_csv = pd.read_csv('../input/train.csv')
test_csv = pd.read_csv('../input/test.csv')

# data nan padding function
def nan_padding(data, columns):
    for column in columns:
        imputer = Imputer()
        data[column] = imputer.fit_transform(data[column].values.reshape(-1,1))
    return data

NaN_columns = ["Age", "SibSp", "Parch"]

train_csv = nan_padding(train_csv, NaN_columns)
test_csv = nan_padding(test_csv, NaN_columns)

train_csv.head()
test_csv.head()

test_passengerId = test_csv["PassengerId"]

# drop not need data
def drop_data(data, columns):
    return data.drop(columns, axis=1)

not_need_data = ["PassengerId", "Name", "Ticket", "Fare", "Cabin", "Embarked"]

train_csv = drop_data(train_csv, not_need_data)
test_csv = drop_data(test_csv, not_need_data)
train_csv.head()

# for one_hot encoding
def dummy_data(data, columns):
    for column in columns:
        data = pd.concat([data, pd.get_dummies(data[column], prefix=column)], axis=1)
        data = data.drop(column, axis=1)
    return data

dummy_column = ["Pclass"]
train_csv = dummy_data(train_csv, dummy_column)
test_csv = dummy_data(test_csv, dummy_column)
train_csv.head()

# Exchange sex to integer
def sex_to_int(data):
    le = LabelEncoder()
    le.fit(["male", "female"])
    data["Sex"] = le.transform(data["Sex"])
    return data

train_csv = sex_to_int(train_csv)
test_csv = sex_to_int(test_csv)
train_csv.head()
test_csv.head()

# Exchange age to 0~1
def age_to_small(data):
    MMSca = MinMaxScaler()
    data["Age"] = MMSca.fit_transform(data["Age"].values.reshape(-1, 1))
    return data

train_csv = age_to_small(train_csv)
test_csv = age_to_small(test_csv)
train_csv.head()

# survive column, train_test data
def split_train_test_data(data, test_sizes=(1-0.8)):
    data_y = data["Survived"]
    lb = LabelBinarizer()
    data_y = lb.fit_transform(data_y)
    data_x = data.drop(["Survived"], axis=1)
    
    train_x, test_x, train_y, test_y = train_test_split(data_x, data_y, test_size=test_sizes)
    return train_x.values, train_y, test_x, test_y

train_x, train_y, test_x, test_y = split_train_test_data(train_csv)

print("train_x:{}".format(train_x.shape))
print("train_y:{}".format(train_y.shape))
print("train_y content:{}".format(train_y[:3]))

print("test_x:{}".format(test_x.shape))
print("test_y:{}".format(test_y.shape))

print(train_x[:5])
print(train_y[:5])

# Build Neural Network
def build_NN(hidden_layer = 10):
    tf.reset_default_graph()
    input_data = tf.placeholder(tf.float32, shape=[None, train_x.shape[1]])
    labels = tf.placeholder(tf.float32, shape=[None, 1])
    learning_rate = tf.placeholder(tf.float32)
    is_running = tf.Variable(True, dtype=tf.bool)
    
    initializer = tf.contrib.layers.xavier_initializer()
    fc1 = tf.layers.dense(input_data, hidden_layer, activation=None, kernel_initializer = initializer)
    fc2 = tf.layers.batch_normalization(fc1, training=is_running)
    fc3 = tf.nn.relu(fc2)
    #out = tf.nn.softmax(fc3)
    
    logits = tf.layers.dense(fc3, 1, activation=None)
    
    cross_entropy = tf.nn.sigmoid_cross_entropy_with_logits(logits=logits, labels=labels)
    cost = tf.reduce_mean(cross_entropy)
    with tf.control_dependencies(tf.get_collection(tf.GraphKeys.UPDATE_OPS)):
        optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate).minimize(cost)
    
    predicted = tf.nn.sigmoid(logits)
    correct_pred = tf.equal(tf.round(predicted), labels)
    accuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float32))
    
    export_nodes = ['input_data', 'labels', 'learning_rate', 'is_running', 'logits', 'cost', 'optimizer', 'predicted', 'accuracy']
    Graph = namedtuple('Graph', export_nodes)
    local_dict = locals()
    
    model = Graph(*[local_dict[each] for each in export_nodes])
    
    return model

model = build_NN()
print(model)

def get_batch(x, y, batch_size=32):
    batch_n = len(x)//batch_size
    for i in range(batch_n):
        batch_x = x[i*batch_size:(i+1)*batch_size]
        batch_y = y[i*batch_size:(i+1)*batch_size]
        
        yield batch_x, batch_y
        
epochs = 200
train_collect = 50
train_print = train_collect * 2

learning_rate_level = 0.01
batch_size = 32

x_collect = []
train_loss_collect = []
train_acc_collect = []
test_loss_collect = []
test_acc_collect = []

saver = tf.train.Saver()
with tf.Session() as sess:
    sess.run(tf.global_variables_initializer())
    iteration = 0
    for e in range(epochs):
        for batch_x, batch_y in get_batch(train_x, train_y, batch_size):
            iteration += 1
            feed = {model.input_data : train_x,
                    model.labels : train_y,
                    model.learning_rate : learning_rate_level,
                    model.is_running : True}
            
            train_loss, _, train_acc = sess.run([model.cost, model.optimizer, model.accuracy], feed_dict = feed)
            
            if(iteration % train_collect == 0):
                x_collect.append(e)
                train_loss_collect.append(train_loss)
                train_acc_collect.append(train_acc)
            
                if(iteration % train_print == 0):
                    print("[Epochs: {}/{}]".format(e+1, epochs),
                         "Train Loss: {:.4f}".format(train_loss),
                         "Train Acc: {:.4f}".format(train_acc))
            
                feed = {model.input_data: test_x,
                       model.labels: test_y,
                       model.is_running : False}
            
                test_loss, test_acc = sess.run([model.cost, model.accuracy], feed_dict = feed)
                test_loss_collect.append(test_loss)
                test_acc_collect.append(test_acc)
            
                if(iteration % train_print == 0):
                    print("[Epochs: {}/{}]".format(e+1, epochs),
                          "Test Loss: {:.4f}".format(test_loss),
                          "Test Acc: {:.4f}".format(test_acc))
    saver.save(sess, "./checkpoint/titanic.ckpt")

model = build_NN()
restorer = tf.train.Saver()
with tf.Session() as sess:
    restorer.restore(sess, "./checkpoint/titanic.ckpt")
    feed = {model.input_data: test_csv,
           model.is_running : False}
    test_predict = sess.run(model.predicted, feed_dict = feed)
binarizer = Binarizer(0.49)
test_predict_result = binarizer.fit_transform(test_predict)
test_predict_result = test_predict_result.astype(np.int32)
print(test_predict_result)

passenger_id = test_passengerId.copy()
evaluation = passenger_id.to_frame()
evaluation["Survived"] = test_predict_result

print(evaluation)

evaluation.to_csv("evaluation_submission.csv", index=False)
